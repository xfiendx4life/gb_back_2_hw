# Урок 7. Асинхрон, брокеры сообщений. Kafka
1. Запустите большее число копий HTTP-сервера. Эмулируйте нагрузку так, чтобы она поступала от балансировщика на все копии сервиса равномерно. Оцените полученные результаты:

* как меняется rps сервиса;
* как изменяется величина задержки.

api были запущены локально с kafka и redis в контейнерах. К сожалению, после большого количства попыток, сервисы не смогли получить доступ к данным брокера из контейнеров, но получали доступ при запуске локально.

__Данные при одном http-ceрвере.__
```bash
Running 1m test @ http://127.0.0.1:8081
  5 threads and 5 connections
  Thread Stats   Avg      Stdev     Max   +/- Stdev
    Latency     1.01s     9.90ms   1.06s    86.78%
    Req/Sec     0.17      0.37     1.00     83.39%
  295 requests in 1.00m, 21.61KB read
Requests/sec:      4.91
Transfer/sec:     368.17B
```

Для балансировки нагрузки был использован сервер nginx. И простейший балансировщик, с конфигурацией:

```nginx
events { worker_connections 1024; }
http {
  upstream myproject {
    server 127.0.0.1:8081;
    server 127.0.0.1:8082;
    server 127.0.0.1:8083;
  }

  server {
    listen 8080;
    server_name www.domain.com;
    location / {
      proxy_pass http://myproject;
    }
  }
}
```

Так как это первый опыт работы с nginx, то я проверил логи и увидел, что данные запросов действительно проходили через него.
```bash
...
127.0.0.1 - - [21/May/2022:23:55:55 +0300] "POST /rate?rate=10 HTTP/1.1" 200 0 "-" "-"
127.0.0.1 - - [21/May/2022:23:55:55 +0300] "POST /rate?rate=10 HTTP/1.1" 200 0 "-" "-"
127.0.0.1 - - [21/May/2022:23:55:56 +0300] "POST /rate?rate=5 HTTP/1.1" 200 0 "-" "-"
127.0.0.1 - - [21/May/2022:23:55:56 +0300] "POST /rate?rate=5 HTTP/1.1" 200 0 "-" "-"
127.0.0.1 - - [21/May/2022:23:55:56 +0300] "POST /rate?rate=1 HTTP/1.1" 200 0 "-" "-"
127.0.0.1 - - [21/May/2022:23:55:56 +0300] "POST /rate?rate=5 HTTP/1.1" 200 0 "-" "-"
127.0.0.1 - - [21/May/2022:23:55:56 +0300] "POST /rate?rate=5 HTTP/1.1" 200 0 "-" "-"
127.0.0.1 - - [21/May/2022:23:55:57 +0300] "POST /rate?rate=1 HTTP/1.1" 200 0 "-" "-"
...
```

__Данные при трех http-серверах.__
```bash
wrk -c5 -t5 -d1m -s ./wrk.lua 'http://127.0.0.1:8080'
Running 1m test @ http://127.0.0.1:8080
  5 threads and 5 connections
  Thread Stats   Avg      Stdev     Max   +/- Stdev
    Latency     1.01s     4.40ms   1.04s    85.42%
    Req/Sec     0.24      0.43     1.00     76.27%
  295 requests in 1.00m, 37.45KB read
Requests/sec:      4.91
Transfer/sec:     638.20B
```

Можно сделать вывод, что задержка и rps не изменились значительно при увеличении количства http-серверов.
